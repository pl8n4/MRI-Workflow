# workflow.conf — edit once, source everywhere

# locate the workflow directory (where this file lives)
WORKFLOW_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

# BIDS dataset root is the parent directory of workflow/
BIDS_ROOT="$( dirname "${WORKFLOW_DIR}" )"
DERIV_ROOT="${BIDS_ROOT}/derivatives"

SIF_IMAGE="${BIDS_ROOT}/mriqc_latest.sif"


# auto-detect subjects
SUBJECT_LIST="$(ls -d "${BIDS_ROOT}"/sub-* 2>/dev/null | sed 's:.*/sub-::')"
TOTAL_SUBJECTS="$(wc -w <<<"${SUBJECT_LIST}")"

# phase‑specific “typical” peak RAM (GB) per job, threads will be set up by optimize_workflow.py
MRIQC_RAM=8
SSW_RAM=6
AFNI_RAM=6

# threads that group‑level AFNI programs may use (NOT read by per‑subject scripts)
AFNI_THREADS=8          # tweak if you want, but current group analysis (t test) runs super quick

# absolute paths to per‑subject runners
RUN_MRIQC="${WORKFLOW_DIR}/run_mriqc.sh"
RUN_SSW="${WORKFLOW_DIR}/run_sswarper.sh"
RUN_AFNI="${WORKFLOW_DIR}/afni_proc.sh"
RUN_GROUP="${WORKFLOW_DIR}/run_group_afni.sh"

# Option to run group mriqc and group analysis respecivley
RUN_MRIQC_GROUP=true
RUN_GROUP_AFNI=true

# Only supports one node at a time, dont use slurm, not set up
# launcher: "local" (=GNU parallel)  or  "slurm"
LAUNCHER=local           # keep this as is
#SLURM_PARTITION=normal   only if LAUNCHER=slurm